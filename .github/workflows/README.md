# AI Agent GitHub Workflows

This directory contains GitHub Actions workflows for managing AI agent activation issues.

## Workflows

### 1. ğŸ¤– AI Agent Issue Management (`ai-agent-issue.yml`)

**Purpose**: Automatically manages the AI agent activation issue to ensure it always exists with the correct label.

**Triggers**:
- â° **Scheduled**: Runs every hour (`0 * * * *`)
- ğŸ”„ **Issue Events**: Responds to issue changes (closed, labeled, unlabeled)
- ğŸ‘† **Manual**: Can be triggered manually via workflow_dispatch

**Logic**:
1. Checks for existing issues with the `ai-agent-active` label
2. If no labeled issue exists, looks for issues with the title "ğŸ¤– AI Agent Activation"
3. Adds the label to existing unlabeled issues
4. Creates a new issue if none exists
5. Validates the final state and reports results

**Features**:
- âœ… Comprehensive validation and error handling
- ğŸ“Š Detailed logging and reporting
- ğŸ”„ Automatic recovery from edge cases
- ğŸ›¡ï¸ Minimal required permissions

### 2. ğŸš€ Create AI Agent Issue (`create-ai-agent-issue.yml`)

**Purpose**: Simplified manual workflow for immediate AI agent issue creation/management.

**Triggers**:
- ğŸ‘† **Manual Only**: workflow_dispatch with optional force creation

**Options**:
- `force_create`: Creates a new issue even if one already exists

**Use Cases**:
- Testing the issue creation process
- Immediate activation of AI agents
- Recovery from workflow failures

### 3. ğŸ§ª Test AI Agent Workflows (`test-ai-agent-workflows.yml`)

**Purpose**: Comprehensive testing suite for validating all workflow scenarios.

**Test Scenarios**:
- **No existing issue**: Tests creation from scratch
- **Issue without label**: Tests label addition
- **Issue with label**: Tests detection of existing valid issues
- **Multiple issues**: Tests handling of complex scenarios
- **Cleanup**: Removes test artifacts

**Usage**:
```bash
# Run all tests
Go to Actions â†’ "Test AI Agent Workflows" â†’ Run workflow â†’ Select "all"

# Run specific test
Go to Actions â†’ "Test AI Agent Workflows" â†’ Run workflow â†’ Select scenario
```

## Configuration

### Labels
- **Primary**: `ai-agent-active` - Required for AI agent activation
- **Test Labels**: Various test-specific labels for isolation

### Issue Title
- **Standard**: `ğŸ¤– AI Agent Activation`
- **Test Variations**: Include "(Test)" suffix for test scenarios

### Permissions Required
```yaml
permissions:
  issues: write
  contents: read
```

## Usage Instructions

### Quick Start
1. **Immediate Activation**: 
   - Go to Actions â†’ "Create AI Agent Issue" â†’ Run workflow
   
2. **Automatic Management**: 
   - The system runs hourly automatically
   - Responds to issue state changes

3. **Testing**:
   - Go to Actions â†’ "Test AI Agent Workflows" â†’ Run workflow
   - Select "all" to run comprehensive tests

### Monitoring
- Check the Actions tab for workflow runs
- Review logs for detailed operation reports
- Monitor issue #[number] for the AI agent activation issue

### Troubleshooting

**Issue doesn't have the label**:
- Wait for the next hourly run, or
- Manually trigger "AI Agent Issue Management"

**Multiple issues exist**:
- The workflow will use the first one found with the label
- Run cleanup via the test workflow if needed

**Permissions errors**:
- Verify the repository has Actions enabled
- Check that the workflow has the required permissions

**Validation failures**:
- Check workflow logs for specific error details
- Ensure the repository allows issue creation
- Verify GitHub token permissions

## Workflow Outputs

Each workflow provides structured outputs:

```yaml
action: "exists" | "labeled" | "created"
issue-number: "123"
issue-url: "https://github.com/owner/repo/issues/123"
validation: "passed" | "failed" | "error"
```

## Testing Strategy

The test workflow covers:

1. **Scenario Coverage**:
   - âœ… No existing issue (creation test)
   - âœ… Issue without label (labeling test)
   - âœ… Issue with label (detection test)
   - âœ… Multiple issues (selection test)

2. **Validation Checks**:
   - Issue existence
   - Correct state (open)
   - Proper labeling
   - Title accuracy

3. **Cleanup**:
   - Automatic test artifact removal
   - Safe isolation of test data

## Best Practices

1. **Regular Testing**: Run the test workflow monthly
2. **Monitor Logs**: Review workflow logs for any warnings
3. **Label Management**: Don't manually remove the `ai-agent-active` label
4. **Issue Preservation**: Keep the AI agent issue open for continuous activation

## Support

For issues with these workflows:
1. Check the workflow logs in the Actions tab
2. Run the test workflow to diagnose problems
3. Review this documentation for configuration details
4. Verify repository permissions and settings